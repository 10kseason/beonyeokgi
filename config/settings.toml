[device]
input_samplerate = 48000
output_samplerate = 48000
input_device = ""
output_device = ""

[asr]
whisper_model = "medium" # recommended: large-v3-turbo for accuracy, medium for lower load
device = "cuda"
compute_type = "float16"  # CUDA acceleration; cpu mode will override to int8
task = "translate"
language = "ko"
beam_size = 2
condition_on_previous_text = false
vad = true

[vad]
frame_ms = 20
aggressiveness = 1
min_speech_sec = 0.18
silence_pad_ms = 320
max_segment_ms = 9000
chunk_enable = false
# ---- Forced Segmentation (when VAD misses speech) ----
[vad.force]
enable = true
rms_speech_threshold_dbfs = -26.5  # >= this is considered speech-like
min_forced_segment_ms = 2000      # minimum duration to cut on loud->quiet
sustained_loud_ms = 7000          # force if loud this long with no VAD end
max_buffer_ms = 20000             # cap buffer


[tts]
engine = "kokoro"
voice = "en-US-AriaNeural"
piper_model = ""
pace = 1.0
volume_db = 10.4

[kokoro]
model = "hexgrad/Kokoro-82M"
speaker = "af_bella"
backend = "pytorch"  # auto | pytorch | onnx
device = "cuda"  # auto -> CUDA when >=12 GiB VRAM detected
use_half = true
output_device = ""  # Optional: override Kokoro playback target (defaults to [device].output_device)
onnx_model = ""
onnx_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
execution_provider = ""
passthrough_input_device = ""  # Optional: mirror Kokoro audio to another device (e.g., virtual mic)
short_threshold_ms = 320.0
min_batch_ms = 500.0
max_batch_ms = 1000.0
medium_min_ms = 750.0
medium_max_ms = 1500.0
crossfade_ms = 120.0
tail_flush_ms = 320.0
short_idle_flush_ms = 500.0
warmup_runs = 2
dynamic_pace = true
dynamic_pace_min = 0.35
dynamic_pace_max = 1.8
dynamic_pace_tolerance = 0.12
dynamic_min_target_ms = 750.0
dynamic_max_target_ms = 20000.0
estimate_max_ms = 20000.0

[app]
preset = "accuracy"
compute_mode = "cuda"

[translator]
use_llm = true
llm_backend = "lmstudio"  # ollama | lmstudio
timeout_sec = 8.0
temperature = 0.2

[translator.ollama]
base_url = "http://localhost:11434"
model = ""

[translator.lmstudio]
base_url = "http://localhost:1234/v1"
model = "qwen/qwen3-4b-2507"

[stream]
mode = "chunk"
normalize_dbfs = -2.5

[voice_changer]
enabled = false
base_url = "http://localhost:18000"
endpoint = "/api/voice-changer/convert_chunk"
input_sample_rate = 0
output_sample_rate = 0
timeout_sec = 5.0
save_original_path = "cache/tts_original.wav"
save_converted_path = "cache/tts_converted.wav"
fallback_endpoint = "/api/voice-changer/convert_chunk_bulk"
fallback_output_device = ""
stream_mode = false
stream_chunk_ms = 1000

[logging]
level = "INFO"



