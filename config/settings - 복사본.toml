[device]
input_samplerate = 48000
output_samplerate = 48000
input_device = ""
output_device = ""

[asr]
whisper_model = "medium" #정확도 높은 방식 :large-v3-turbo #성능 방식  : medium #모델 알아서 선택하세요
device = "cuda"
compute_type = "int8"
task = "translate"
language = "ko"
beam_size = 1
temperature = 0.0
vad = true

[vad]
frame_ms = 20
aggressiveness = 2
min_speech_sec = 0.30
silence_end_ms = 400

[tts]
engine = "kokoro"
voice = "en-US-AriaNeural"
piper_model = ""
pace = 1.0
volume_db = 10.3

[kokoro]
model = "hexgrad/Kokoro-82M"
speaker = "af_bella"
backend = "auto"  # auto | pytorch | onnx
device = "auto"
use_half = true
onnx_model = ""
onnx_providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
execution_provider = ""
passthrough_input_device = ""  # Optional: mirror Kokoro audio to another device (e.g., virtual mic)
short_threshold_ms = 320.0
min_batch_ms = 500.0
max_batch_ms = 1000.0
medium_min_ms = 750.0
medium_max_ms = 1500.0
crossfade_ms = 120.0
tail_flush_ms = 320.0
short_idle_flush_ms = 500.0
warmup_runs = 2

[app]
preset = "latency"

[stream]
mode = "chunk"
normalize_dbfs = -16

[voice_changer]
enabled = false
base_url = "http://localhost:18000"
endpoint = "/api/voice-changer/convert_chunk"
input_sample_rate = 0
output_sample_rate = 0
timeout_sec = 5.0
save_original_path = "cache/tts_original.wav"
save_converted_path = "cache/tts_converted.wav"
fallback_endpoint = "/api/voice-changer/convert_chunk_bulk"
fallback_output_device = ""
stream_mode = false
stream_chunk_ms = 1000

[logging]
level = "INFO"
